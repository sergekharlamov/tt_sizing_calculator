{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports, constants and basic structures\n",
    "import csv, json\n",
    "from functools import reduce\n",
    "from distutils import util\n",
    "from math import ceil\n",
    "\n",
    "server = {'ram': 64 * 1024**3, 'cpu':16, 'ssd':256 * 1024**3, 'hdd': 256 * 1024**3}\n",
    "\n",
    "# Should be input by a user, but using a constant for now (with defaults for checkpoint configs)\n",
    "rps = 5000\n",
    "checkpoint_count = 2\n",
    "checkpoint_interval = 3600\n",
    "\n",
    "# Storage capacity of a single storage Instance (in Gb)\n",
    "single_storage_size = 30\n",
    "\n",
    "# box.cfg and other Tarantool constants\n",
    "slab_alloc_minimal = 16\n",
    "slab_alloc_granularity = 8\n",
    "index_init_insert = 48 * 1024   # initial cost of index creation\n",
    "tuple_size = 10                 # since 2.10\n",
    "large_tuple_size = 14           # legacy and large tuples\n",
    "tuple_str_header = 5            # msgpack str overhead\n",
    "\n",
    "# Размеры типов в msgpack\n",
    "types = {'boolean':{'size': 1, 'indexes':('tree', 'hash')},\n",
    "         'integer': {'size': 9, 'indexes':('tree', 'hash')},\n",
    "         'unsigned':{'size': 9, 'indexes':('tree', 'hash', 'bitset')},\n",
    "         'double':{'size': 9, 'indexes':('tree', 'hash')},\n",
    "         'number':{'size': 9, 'indexes':('tree', 'hash')},\n",
    "         'decimal':{'size': 8, 'indexes':('tree', 'hash')},\n",
    "         'string':{'size': 5, 'indexes':('tree', 'hash', 'bitset')}, \n",
    "         'varbinary':{'size': 5, 'indexes':('tree', 'hash', 'bitset')},\n",
    "         'uuid':{'size': 16, 'indexes':('tree', 'hash')},\n",
    "         'array':{'size': 5, 'indexes':('rtree')},\n",
    "         'map':{'size': 5, 'indexes':('tree', 'hash')}}\n",
    "\n",
    "# Базовые размеры ключей \n",
    "indexes = {'tree':{'key': 20},\n",
    "            'hash':{'key': 16}}\n",
    "\n",
    "# Object for storing basic data on Field\n",
    "class Field:\n",
    "    def __init__(self, opts):\n",
    "        self.name = opts['name']\n",
    "        self.type = opts['type']\n",
    "        self.len = int(opts['strlen']) + int(types[opts['type']]['size']) if self.type in (\"string\", \"varbinary\") else types[opts['type']]['size']\n",
    "        self.indexed = util.strtobool(opts['indexed'])\n",
    "        self.indexes = ((), types[opts['type']]['indexes'])[bool(opts['indexed'])]\n",
    "        \n",
    "    def __str__(self):\n",
    "        return str({\"Name\": self.name, \"Is indexed\": self.indexed})\n",
    "\n",
    "\n",
    "def roundup_by(x, multiple):\n",
    "    x = int(x); multiple = int(multiple)\n",
    "    if multiple == 0 and not x % multiple:\n",
    "        return x\n",
    "    return x + multiple - x % multiple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Space' class for sizing calculation \n",
    "class Space:\n",
    "    row_size = 0\n",
    "\n",
    "    def __init__(self, name, rows, sharded):\n",
    "        self.name = name\n",
    "        self.fields = []\n",
    "        self.rows = int(rows)\n",
    "        self.sharded = util.strtobool(sharded)\n",
    "    \n",
    "    def add_field(self, opts):\n",
    "        if isinstance(opts, list):\n",
    "            self.fields.extend(opts)\n",
    "        else:\n",
    "            self.fields.append(opts)\n",
    "\n",
    "    def __str__(self):\n",
    "        return str({\"Name\": self.name, \"Rows count:\": self.rows,\n",
    "                    \"Is sharded\": ('false', 'true')[self.sharded], \"Fields\": list(map(lambda x: str(x), self.fields))})\n",
    " \n",
    "    # Method for calculating space size (data + keys) (Counts only tree indexes)\n",
    "    def calculate_size(self):\n",
    "        # Если спейс шардированный добавим поле для bucket_id\n",
    "        self.fields.append(Field({\"name\":\"bucketid\", \"type\":\"number\", \"indexed\": \"true\"}))\n",
    "        \n",
    "        # Посчитать длину всех полей и добавить хедер\n",
    "        row_item_size = reduce(lambda x, y: x + y , map(lambda x: x.len ,self.fields)) + tuple_size\n",
    "        \n",
    "        # Посчитать оффсеты для индексов\n",
    "        tuple_offsets = len(list(filter(lambda x: x.indexed ,self.fields))) - 1\n",
    "        (0, tuple_offsets * 4)[tuple_offsets > 0]\n",
    "        \n",
    "        # Добавляем выравнивание по slab_alloc_minimal и slab_alloc_granularity\n",
    "        tuple_arena = roundup_by(roundup_by(row_item_size + tuple_offsets, slab_alloc_minimal),slab_alloc_granularity) * self.rows\n",
    "        \n",
    "        # Считаем размер ключей\n",
    "        row_keys_size = reduce(lambda x, y: x + y, \n",
    "                            map(lambda x: indexes[x.indexes[0]]['key'] + index_init_insert, \n",
    "                            filter(lambda x: x.indexed ,self.fields)))\n",
    "        self.row_size = tuple_arena + row_keys_size\n",
    "\n",
    "        return self.row_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and parse sample data from box.csv\n",
    "box = {}\n",
    "\n",
    "with open('box.csv') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    sample_data = list(reader)\n",
    "    for field in sample_data:\n",
    "        if field['space'] in box.keys():\n",
    "            box[field['space']].add_field(Field(field))\n",
    "        else:\n",
    "            new_space = Space(field['space'], field['size'], field['sharded'])\n",
    "            new_space.add_field(Field(field))\n",
    "            box[new_space.name] = new_space\n",
    "\n",
    "print(\"Imported a box with following spaces:\")\n",
    "for space in box: print(box[space])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sizing in Gb (Calculates: RAM)\n",
    "# Fill the box.csv and play this cell\n",
    "total_ram_size_gb = reduce(lambda x,y : x + y, map(lambda x: box[x].calculate_size(), box)) / 1024**3\n",
    "\n",
    "# HDD stores (total_ram_size_gb * checkpoint_count) Gb for checkpoints and sum of (Space.row_size * rps * checkpoint_interval) for each space for WAL\n",
    "total_hdd_size_gb = (total_ram_size_gb * checkpoint_count) + reduce(lambda x,y : x + y, map(lambda x: box[x].row_size * rps * checkpoint_interval, box)) / 1024**3\n",
    "\n",
    "# Calculates CPU for cluster\n",
    "total_cpu = ceil(total_ram_size_gb / single_storage_size) * 2 + ceil(float(rps) / 5000) \n",
    "\n",
    "print(\"The Total Cluster Size in RAM is:\", total_ram_size_gb)\n",
    "print(\"The Total HDD needed:\", total_hdd_size_gb)\n",
    "print(\"The Total CPU needed for the cluster:\", total_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, let's try to parse Tarantool Avro\n",
    "with open(\"tdg_model.avsc\") as avro_model:\n",
    "    model = json.load(avro_model)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
